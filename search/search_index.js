var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>This site contains the documentation for the <code>behavysis_pipeline</code> program.</p>"},{"location":"examples/batch_pipeline.html","title":"Analysing a Folder of Experiments","text":"<p>All outcomes for experiment processing is stored in csv files in the <code>proj_dir/diagnostics</code> folder. These files store the outcome and process description (i.e. error explanations) of all experiments.</p>"},{"location":"examples/batch_pipeline.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>from behavysis_pipeline import Project\nfrom behavysis_pipeline.processes import *\n</code></pre> <p>TODO</p>"},{"location":"examples/single_pipeline.html","title":"Single Image Analysis Pipeline","text":""},{"location":"examples/single_pipeline.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>from microscopy_proc.pipelines.pipeline_funcs import (\n    cell_mapping_pipeline,\n    cellc1_pipeline,\n    cellc2_pipeline,\n    cellc3_pipeline,\n    cellc4_pipeline,\n    cellc5_pipeline,\n    cellc6_pipeline,\n    cellc7_pipeline,\n    cellc8_pipeline,\n    cellc9_pipeline,\n    cellc10_pipeline,\n    cellc11_pipeline,\n    cellc_coords_only_pipeline,\n    cells2csv_pipeline,\n    coords2points_raw_pipeline,\n    coords2points_trfm_pipeline,\n    group_cells_pipeline,\n    img_fine_pipeline,\n    img_overlap_pipeline,\n    img_rough_pipeline,\n    img_trim_pipeline,\n    make_mask_pipeline,\n    ref_prepare_pipeline,\n    registration_pipeline,\n    tiff2zarr_pipeline,\n    transform_coords_pipeline,\n)\nfrom microscopy_proc.utils.proj_org_utils import (\n    get_proj_fp_model,\n    update_configs,\n)\n</code></pre>"},{"location":"examples/single_pipeline.html#set-the-input-filenamefolder-and-project-folder-to-use","title":"Set the input filename/folder and project folder to use","text":"<pre><code>in_fp = \"/path/to/input_file.tiff\"\nproj_dir = \"/path/to/proj_dir\"\n</code></pre>"},{"location":"examples/single_pipeline.html#load-project-object","title":"Load project object","text":"<pre><code>pfm = get_proj_fp_model(proj_dir)\n</code></pre>"},{"location":"examples/single_pipeline.html#update-configs","title":"Update configs","text":"<p>Also see Configs for more parameters to modify and their descriptions.</p> <pre><code>update_configs(\n    pfm,\n    # REFERENCE\n    # RAW\n    # REGISTRATION\n    ref_orient_ls=(-2, 3, 1),\n    ref_z_trim=(None, None, None),\n    ref_y_trim=(None, None, None),\n    ref_x_trim=(None, None, None),\n    z_rough=3,\n    y_rough=6,\n    x_rough=6,\n    z_fine=1,\n    y_fine=0.6,\n    x_fine=0.6,\n    z_trim=(None, None, None),\n    y_trim=(None, None, None),\n    x_trim=(None, None, None),\n    # MASK\n    # OVERLAP\n    # CELL COUNTING\n    tophat_sigma=10,\n    dog_sigma1=1,\n    dog_sigma2=4,\n    gauss_sigma=101,\n    thresh_p=60,\n    min_threshd=100,\n    max_threshd=9000,\n    maxima_sigma=10,\n    min_wshed=1,\n    max_wshed=700,\n)\n</code></pre>"},{"location":"examples/single_pipeline.html#run-pipeline","title":"Run Pipeline","text":"<p>For more information, see Pipeline</p>"},{"location":"examples/single_pipeline.html#single-line-version","title":"Single line version","text":"<pre><code>all_pipeline(pfm, in_fp, overwrite=overwrite)\n</code></pre>"},{"location":"examples/single_pipeline.html#manually-run-each-function-version","title":"Manually run each function version","text":"<pre><code># Making zarr from tiff file(s)\ntiff2zarr_pipeline(pfm, in_fp, overwrite=overwrite)\n# Preparing reference images\nref_prepare_pipeline(pfm, overwrite=overwrite)\n# Preparing image itself\nimg_rough_pipeline(pfm, overwrite=overwrite)\nimg_fine_pipeline(pfm, overwrite=overwrite)\nimg_trim_pipeline(pfm, overwrite=overwrite)\n# Running Elastix registration\nregistration_pipeline(pfm, overwrite=overwrite)\n# Running mask pipeline\nmake_mask_pipeline(pfm, overwrite=overwrite)\n# Making overlap chunks in preparation for cell counting\nimg_overlap_pipeline(pfm, overwrite=overwrite)\n# Counting cells\ncellc1_pipeline(pfm, overwrite=overwrite)\ncellc2_pipeline(pfm, overwrite=overwrite)\ncellc3_pipeline(pfm, overwrite=overwrite)\ncellc4_pipeline(pfm, overwrite=overwrite)\ncellc5_pipeline(pfm, overwrite=overwrite)\ncellc6_pipeline(pfm, overwrite=overwrite)\ncellc7_pipeline(pfm, overwrite=overwrite)\ncellc8_pipeline(pfm, overwrite=overwrite)\ncellc9_pipeline(pfm, overwrite=overwrite)\ncellc10_pipeline(pfm, overwrite=overwrite)\ncellc11_pipeline(pfm, overwrite=overwrite)\ncellc_coords_only_pipeline(pfm, overwrite=overwrite)\n# Converting maxima from raw space to refernce atlas space\ntransform_coords_pipeline(pfm, overwrite=overwrite)\n# Getting Region ID mappings for each cell\ncell_mapping_pipeline(pfm, overwrite=overwrite)\n# Grouping cells\ngroup_cells_pipeline(pfm, overwrite=overwrite)\n# Exporting cells_agg parquet as csv\ncells2csv_pipeline(pfm, overwrite=overwrite)\n</code></pre>"},{"location":"examples/single_pipeline.html#optional-run-visual-checks","title":"Optional: Run visual checks","text":"<pre><code># Running visual checks\ncoords2points_raw_pipeline(pfm)\ncoords2points_trfm_pipeline(pfm)\n</code></pre>"},{"location":"installation/installing.html","title":"Installing","text":"<p>Step 1:</p> <p>Install conda by visiting the Miniconda downloads page and following the prompts to install on your system.</p> <p>Open the downloaded miniconda file and follow the installation prompts.</p> <p>Step 2:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows) and verify that conda has been installed with the following command.</p> <pre><code>conda --version\n</code></pre> <p>A response like <code>conda xx.xx.xx</code> indicates that it has been correctly installed.</p> <p>Step 3:</p> <p>Update conda and use the libmamba solver (makes downloading conda programs MUCH faster):</p> <pre><code>conda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre> <p>Step 4:</p> <p>Install packages that help Jupyter notebooks read conda environments:</p> <pre><code>conda install -n base nb_conda nb_conda_kernels\n</code></pre> <p>Step 5:</p> <p>Install the <code>behavysis_pipeline</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/conda_env.yaml\n</code></pre> <p>Step 6:</p> <p>Install the <code>DEEPLABCUT</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/DEEPLABCUT.yaml\n</code></pre> <p>Step 7:</p> <p>Install the <code>simba</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/simba_env.yaml\n</code></pre>"},{"location":"installation/running.html","title":"Running","text":"<p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>Activate the program environment with the following command:</p> <pre><code>conda activate behavysis_pipeline_env\n</code></pre> <p>Step 3:</p> <p>You can now use the <code>behavysis</code> package in a Jupyter kernel or regular Python script.</p> <p>See here for examples of Jupyter notebooks to run behaviour analysis.</p> <p>See here for examples of Jupyter notebooks to train behaviour classifiers.</p> <p>See here for a tutorial of <code>behavysis</code>'s workflow.</p> <p>See here for API documentation.</p> <p>Note</p> <p>To run jupyter, run the following command in the terminal</p> <pre><code>jupyter-lab\n</code></pre> <p>This will open a browser to <code>http://127.0.0.1:8888/lab</code>, where you can run jupyter notebooks.</p> <p>You can also run jupyter notebooks in VS Code.</p>"},{"location":"installation/uninstalling.html","title":"Uninstalling","text":"<p>For more information about how to uninstall conda, see here.</p> <p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>To uninstall the <code>behavysis_pipeline_env</code>, <code>DEEPLABCUT</code>, and <code>simba</code> conda envs, run the following commands:</p> <pre><code>conda env remove -n behavysis_pipeline_env\nconda env remove -n DEEPLABCUT\nconda env remove -n simba\n</code></pre> <p>Step 3:</p> <p>To remove conda, enter the following commands in the terminal.</p> <pre><code>conda install anaconda-clean\nanaconda-clean --yes\n\nrm -rf ~/anaconda3\nrm -rf ~/opt/anaconda3\nrm -rf ~/.anaconda_backup\n</code></pre> <p>Step 5: Edit your bash or zsh profile so conda it does not look for conda anymore. Open each of these files (note that not all of them may exist on your computer), <code>~/.zshrc</code>, <code>~/.zprofile</code>, or <code>~/.bash_profile</code>, with the following command.</p> <pre><code>open ~/.zshrc\nopen ~/.zprofile\nopen ~/.bash_profile\n</code></pre>"},{"location":"installation/updating.html","title":"Updating","text":"<p>Step 1: Download the <code>conda_env.yaml</code> file from here</p> <p>Step 2: Run the following command to update <code>behavysis_pipeline</code>:</p> <pre><code>conda env update -f conda_env.yaml --prune\n</code></pre>"},{"location":"reference/config_params_model.html","title":"Config params model","text":""},{"location":"reference/config_params_model.html#microscopy_proc.utils.config_params_model.ConfigParamsModel","title":"<code>microscopy_proc.utils.config_params_model.ConfigParamsModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for registration parameters.</p> Source code in <code>microscopy_proc/utils/config_params_model.py</code> <pre><code>class ConfigParamsModel(BaseModel):\n    \"\"\"\n    Pydantic model for registration parameters.\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n        # arbitrary_types_allowed=True,\n        validate_default=True,\n        use_enum_values=True,\n    )\n\n    # REFERENCE\n    atlas_dir: str = RESOURCES_DIR\n    ref_v: RefVersions = RefVersions.AVERAGE_TEMPLATE_25\n    annot_v: AnnotVersions = AnnotVersions.CCF_2016_25\n    map_v: MapVersions = MapVersions.ABA_ANNOTATIONS\n    # RAW\n    chunksize: tuple[int, int, int] = PROC_CHUNKS\n    # REGISTRATION\n    ref_orient_ls: tuple[int, int, int] = (1, 2, 3)\n    ref_z_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    ref_y_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    ref_x_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    z_rough: int = 3\n    y_rough: int = 6\n    x_rough: int = 6\n    z_fine: float = 1.0\n    y_fine: float = 0.6\n    x_fine: float = 0.6\n    z_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    y_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    x_trim: tuple[int | None, int | None, int | None] = (None, None, None)\n    # MASK\n    mask_gaus_blur: int = 1\n    mask_thresh: int = 300\n    # OVERLAP\n    depth: int = DEPTH\n    # CELL COUNTING\n    tophat_sigma: int = 10\n    dog_sigma1: int = 1\n    dog_sigma2: int = 4\n    gauss_sigma: int = 101\n    thresh_p: int = 60\n    min_threshd: int = 100\n    max_threshd: int = 10000\n    maxima_sigma: int = 10\n    min_wshed: int = 1\n    max_wshed: int = 1000\n\n    @model_validator(mode=\"after\")\n    def validate_trims(self):\n        # Orient validation\n        ref_orient_ls_abs = [abs(i) for i in self.ref_orient_ls]\n        assert max(ref_orient_ls_abs) == 3\n        assert min(ref_orient_ls_abs) == 1\n        assert sum(ref_orient_ls_abs) == 6\n        # Size validation\n        # Trim validation\n        return self\n\n    def update(self, **kwargs):\n        return self.model_validate(self.model_copy(update=kwargs))\n\n    @classmethod\n    def update_file(cls, fp: str, **kwargs):\n        \"\"\"\n        Reads the json file in `fp`, updates the parameters with `kwargs`,\n        writes the updated parameters back to `fp` (if there are any updates),\n        and returns the model instance.\n        \"\"\"\n        configs = cls.model_validate(read_json(fp))\n        # Updating and saving if kwargs is not empty\n        if kwargs != {}:\n            configs = cls.model_validate(configs.model_copy(update=kwargs))\n            write_json(fp, configs.model_dump())\n        return configs\n</code></pre>"},{"location":"reference/config_params_model.html#microscopy_proc.utils.config_params_model.ConfigParamsModel.update_file","title":"<code>update_file(fp, **kwargs)</code>  <code>classmethod</code>","text":"<p>Reads the json file in <code>fp</code>, updates the parameters with <code>kwargs</code>, writes the updated parameters back to <code>fp</code> (if there are any updates), and returns the model instance.</p> Source code in <code>microscopy_proc/utils/config_params_model.py</code> <pre><code>@classmethod\ndef update_file(cls, fp: str, **kwargs):\n    \"\"\"\n    Reads the json file in `fp`, updates the parameters with `kwargs`,\n    writes the updated parameters back to `fp` (if there are any updates),\n    and returns the model instance.\n    \"\"\"\n    configs = cls.model_validate(read_json(fp))\n    # Updating and saving if kwargs is not empty\n    if kwargs != {}:\n        configs = cls.model_validate(configs.model_copy(update=kwargs))\n        write_json(fp, configs.model_dump())\n    return configs\n</code></pre>"},{"location":"reference/pipeline_funcs.html","title":"Pipeline funcs","text":""},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs","title":"<code>microscopy_proc.pipelines.pipeline_funcs</code>","text":""},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.all_pipeline","title":"<code>all_pipeline(in_fp, pfm, overwrite=False)</code>","text":"<p>Running all pipelines in order.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>def all_pipeline(\n    in_fp: str,\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Running all pipelines in order.\n    \"\"\"\n    # Running all pipelines in order\n    tiff2zarr_pipeline(pfm, in_fp, overwrite=overwrite)\n    ref_prepare_pipeline(pfm, overwrite=overwrite)\n    img_rough_pipeline(pfm, overwrite=overwrite)\n    img_fine_pipeline(pfm, overwrite=overwrite)\n    img_trim_pipeline(pfm, overwrite=overwrite)\n    registration_pipeline(pfm, overwrite=overwrite)\n    make_mask_pipeline(pfm, overwrite=overwrite)\n    img_overlap_pipeline(pfm, overwrite=overwrite)\n    cellc1_pipeline(pfm, overwrite=overwrite)\n    cellc2_pipeline(pfm, overwrite=overwrite)\n    cellc3_pipeline(pfm, overwrite=overwrite)\n    cellc4_pipeline(pfm, overwrite=overwrite)\n    cellc5_pipeline(pfm, overwrite=overwrite)\n    cellc6_pipeline(pfm, overwrite=overwrite)\n    cellc7_pipeline(pfm, overwrite=overwrite)\n    cellc8_pipeline(pfm, overwrite=overwrite)\n    cellc9_pipeline(pfm, overwrite=overwrite)\n    cellc10_pipeline(pfm, overwrite=overwrite)\n    cellc11_pipeline(pfm, overwrite=overwrite)\n    cellc_coords_only_pipeline(pfm, overwrite=overwrite)\n    transform_coords_pipeline(pfm, overwrite=overwrite)\n    cell_mapping_pipeline(pfm, overwrite=overwrite)\n    group_cells_pipeline(pfm, overwrite=overwrite)\n    cells2csv_pipeline(pfm, overwrite=overwrite)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cell_mapping_pipeline","title":"<code>cell_mapping_pipeline(pfm, overwrite=False)</code>","text":"<p>Using the transformed cell coordinates, get the region ID and name for each cell corresponding to the reference atlas.</p> <p>NOTE: saves the cells dataframe as pandas parquet.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cell_mapping_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Using the transformed cell coordinates, get the region ID and name for each cell\n    corresponding to the reference atlas.\n\n    NOTE: saves the cells dataframe as pandas parquet.\n    \"\"\"\n    # Getting region for each detected cell (i.e. row) in cells_df\n    with cluster_proc_contxt(LocalCluster()):\n        # Reading cells_raw and cells_trfm dataframes\n        cells_df = dd.read_parquet(pfm.cells_raw_df).compute()\n        coords_trfm = pd.read_parquet(pfm.cells_trfm_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        coords_trfm = sanitise_smb_df(coords_trfm)\n        # Making unique incrementing index\n        cells_df = cells_df.reset_index(drop=True)\n        # Setting the transformed coords\n        cells_df[f\"{Coords.Z.value}_{TRFM}\"] = coords_trfm[Coords.Z.value].values\n        cells_df[f\"{Coords.Y.value}_{TRFM}\"] = coords_trfm[Coords.Y.value].values\n        cells_df[f\"{Coords.X.value}_{TRFM}\"] = coords_trfm[Coords.X.value].values\n\n        # Reading annotation image\n        annot_arr = tifffile.imread(pfm.annot)\n        # Getting the annotation ID for every cell (zyx coord)\n        # Getting transformed coords (that are within tbe bounds_arr, and their corresponding idx)\n        s = annot_arr.shape\n        trfm_loc = (\n            cells_df[\n                [\n                    f\"{Coords.Z.value}_{TRFM}\",\n                    f\"{Coords.Y.value}_{TRFM}\",\n                    f\"{Coords.X.value}_{TRFM}\",\n                ]\n            ]\n            .round(0)\n            .astype(np.int32)\n            .query(\n                f\"\"\"\n                ({Coords.Z.value}_{TRFM} &gt;= 0) &amp; ({Coords.Z.value}_{TRFM} &lt; {s[0]}) &amp;\n                ({Coords.Y.value}_{TRFM} &gt;= 0) &amp; ({Coords.Y.value}_{TRFM} &lt; {s[1]}) &amp;\n                ({Coords.X.value}_{TRFM} &gt;= 0) &amp; ({Coords.X.value}_{TRFM} &lt; {s[2]})\n                \"\"\"\n            )\n        )\n        # Getting the pixel values of each valid transformed coord (hence the specified index)\n        # By complex array indexing on ar_annot's (z, y, x) dimensions.\n        # nulls are imputed with -1\n        cells_df[AnnotColumns.ID.value] = pd.Series(\n            annot_arr[*trfm_loc.values.T].astype(np.uint32),\n            index=trfm_loc.index,\n        ).fillna(-1)\n\n        # Reading annotation mappings dataframe\n        annot_df = annot_dict2df(read_json(pfm.map))\n        # Getting the annotation name for every cell (zyx coord)\n        cells_df = df_map_ids(cells_df, annot_df)\n        # Saving to disk\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_df = dd.from_pandas(cells_df)\n        cells_df.to_parquet(pfm.cells_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc10_pipeline","title":"<code>cellc10_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 10</p> <p>Trimming filtered regions overlaps to make: - Trimmed maxima image - Trimmed threshold image - Trimmed watershed image</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc10_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 10\n\n    Trimming filtered regions overlaps to make:\n    - Trimmed maxima image\n    - Trimmed threshold image\n    - Trimmed watershed image\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n        # Declaring processing instructions\n        maxima_final_arr = da_trim(maxima_arr, d=configs.depth)\n        threshd_final_arr = da_trim(threshd_filt_arr, d=configs.depth)\n        wshed_final_arr = da_trim(wshed_volumes_arr, d=configs.depth)\n        # Computing and saving\n        disk_cache(maxima_final_arr, pfm.maxima_final)\n        disk_cache(threshd_final_arr, pfm.threshd_final)\n        disk_cache(wshed_final_arr, pfm.wshed_final)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc11_pipeline","title":"<code>cellc11_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 11</p> <p>From maxima and watershed, save the cells.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc11_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 11\n\n    From maxima and watershed, save the cells.\n    \"\"\"\n    with cluster_proc_contxt(LocalCluster(n_workers=2, threads_per_worker=1)):\n        # n_workers=2\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        raw_arr = da.from_zarr(pfm.raw)\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_arr = da.from_zarr(pfm.maxima)\n        wshed_filt_arr = da.from_zarr(pfm.wshed_filt)\n        # Declaring processing instructions\n        # Getting maxima coords and cell measures in table\n        cells_df = block2coords(\n            Cf.get_cells,\n            raw_arr,\n            overlap_arr,\n            maxima_arr,\n            wshed_filt_arr,\n            configs.depth,\n        )\n        # Filtering out by volume (same filter cellc9_pipeline volume_filter)\n        cells_df = cells_df.query(\n            f\"\"\"\n            ({CellColumns.VOLUME.value} &gt;= {configs.min_wshed}) &amp;\n            ({CellColumns.VOLUME.value} &lt;= {configs.max_wshed})\n            \"\"\"\n        )\n        # Computing and saving as parquet\n        cells_df.to_parquet(pfm.cells_raw_df, overwrite=True)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc1_pipeline","title":"<code>cellc1_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 1</p> <p>Top-hat filter (background subtraction)</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc1_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 1\n\n    Top-hat filter (background subtraction)\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        # Declaring processing instructions\n        bgrm_arr = da.map_blocks(\n            Gf.tophat_filt,\n            overlap_arr,\n            configs.tophat_sigma,\n        )\n        # Computing and saving\n        bgrm_arr = disk_cache(bgrm_arr, pfm.bgrm)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc2_pipeline","title":"<code>cellc2_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 2</p> <p>Difference of Gaussians (edge detection)</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc2_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 2\n\n    Difference of Gaussians (edge detection)\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        bgrm_arr = da.from_zarr(pfm.bgrm)\n        # Declaring processing instructions\n        dog_arr = da.map_blocks(\n            Gf.dog_filt,\n            bgrm_arr,\n            configs.dog_sigma1,\n            configs.dog_sigma2,\n        )\n        # Computing and saving\n        dog_arr = disk_cache(dog_arr, pfm.dog)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc3_pipeline","title":"<code>cellc3_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 3</p> <p>Gaussian subtraction with large sigma for adaptive thresholding</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc3_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 3\n\n    Gaussian subtraction with large sigma for adaptive thresholding\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        dog_arr = da.from_zarr(pfm.dog)\n        # Declaring processing instructions\n        adaptv_arr = da.map_blocks(\n            Gf.gauss_subt_filt,\n            dog_arr,\n            configs.gauss_sigma,\n        )\n        # Computing and saving\n        adaptv_arr = disk_cache(adaptv_arr, pfm.adaptv)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc4_pipeline","title":"<code>cellc4_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 4</p> <p>Mean thresholding with standard deviation offset</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc4_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 4\n\n    Mean thresholding with standard deviation offset\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # # Visually inspect sd offset\n        # t_p =adaptv_arr.sum() / (np.prod(adaptv_arr.shape) - (adaptv_arr == 0).sum())\n        # t_p = t_p.compute()\n        # logging.debug(t_p)\n        # Reading input images\n        adaptv_arr = da.from_zarr(pfm.adaptv)\n        # Declaring processing instructions\n        threshd_arr = da.map_blocks(\n            Cf.manual_thresh,\n            adaptv_arr,\n            configs.thresh_p,\n        )\n        # Computing and saving\n        threshd_arr = disk_cache(threshd_arr, pfm.threshd)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc5_pipeline","title":"<code>cellc5_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 5</p> <p>Getting object sizes</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc5_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 5\n\n    Getting object sizes\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster(n_workers=6, threads_per_worker=1)):\n        # Reading input images\n        threshd_arr = da.from_zarr(pfm.threshd)\n        # Declaring processing instructions\n        threshd_volumes_arr = da.map_blocks(\n            Cf.label_with_volumes,\n            threshd_arr,\n        )\n        # Computing and saving\n        threshd_volumes_arr = disk_cache(threshd_volumes_arr, pfm.threshd_volumes)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc6_pipeline","title":"<code>cellc6_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 6</p> <p>Filter out large objects (likely outlines, not cells)</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc6_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 6\n\n    Filter out large objects (likely outlines, not cells)\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        threshd_volumes_arr = da.from_zarr(pfm.threshd_volumes)\n        # Declaring processing instructions\n        threshd_filt_arr = da.map_blocks(\n            Cf.volume_filter,\n            threshd_volumes_arr,\n            configs.min_threshd,\n            configs.max_threshd,\n        )\n        # Computing and saving\n        threshd_filt_arr = disk_cache(threshd_filt_arr, pfm.threshd_filt)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc7_pipeline","title":"<code>cellc7_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 7</p> <p>Get maxima of image masked by labels.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc7_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 7\n\n    Get maxima of image masked by labels.\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        maxima_arr = da.map_blocks(\n            Gf.get_local_maxima,\n            overlap_arr,\n            configs.maxima_sigma,\n            threshd_filt_arr,\n        )\n        # Computing and saving\n        maxima_arr = disk_cache(maxima_arr, pfm.maxima)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc8_pipeline","title":"<code>cellc8_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 8</p> <p>Watershed segmentation volumes.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc8_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 8\n\n    Watershed segmentation volumes.\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster(n_workers=3, threads_per_worker=1)):\n        # n_workers=2\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        wshed_volumes_arr = da.map_blocks(\n            Cf.wshed_segm_volumes,\n            overlap_arr,\n            maxima_arr,\n            threshd_filt_arr,\n        )\n        # Computing and saving\n        wshed_volumes_arr = disk_cache(wshed_volumes_arr, pfm.wshed_volumes)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc9_pipeline","title":"<code>cellc9_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 9</p> <p>Filter out large watershed objects (again likely outlines, not cells).</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc9_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 9\n\n    Filter out large watershed objects (again likely outlines, not cells).\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n        # Declaring processing instructions\n        wshed_filt_arr = da.map_blocks(\n            Cf.volume_filter,\n            wshed_volumes_arr,\n            configs.min_wshed,\n            configs.max_wshed,\n        )\n        # Computing and saving\n        wshed_filt_arr = disk_cache(wshed_filt_arr, pfm.wshed_filt)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc_coords_only_pipeline","title":"<code>cellc_coords_only_pipeline(pfm, overwrite=False)</code>","text":"<p>Get maxima coords. Very basic but faster version of cellc11_pipeline get_cells.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc_coords_only_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Get maxima coords.\n    Very basic but faster version of cellc11_pipeline get_cells.\n    \"\"\"\n    # Reading filtered and maxima images (trimmed - orig space)\n    with cluster_proc_contxt(LocalCluster(n_workers=6, threads_per_worker=1)):\n        # Read filtered and maxima images (trimmed - orig space)\n        maxima_final_arr = da.from_zarr(pfm.maxima_final)\n        # Declaring processing instructions\n        # Storing coords of each maxima in df\n        coords_df = block2coords(\n            Gf.get_coords,\n            maxima_final_arr,\n        )\n        # Computing and saving as parquet\n        coords_df.to_parquet(pfm.maxima_df, overwrite=True)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.group_cells_pipeline","title":"<code>group_cells_pipeline(pfm, overwrite=False)</code>","text":"<p>Grouping cells by region name and aggregating total cell volume and cell count for each region.</p> <p>NOTE: saves the cells_agg dataframe as pandas parquet.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef group_cells_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Grouping cells by region name and aggregating total cell volume\n    and cell count for each region.\n\n    NOTE: saves the cells_agg dataframe as pandas parquet.\n    \"\"\"\n    # Making cells_agg_df\n    with cluster_proc_contxt(LocalCluster()):\n        # Reading cells dataframe\n        cells_df = pd.read_parquet(pfm.cells_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        # Grouping cells by region name\n        cells_agg_df = cells_df.groupby(AnnotColumns.ID.value).agg(CELL_AGG_MAPPINGS)\n        cells_agg_df.columns = list(CELL_AGG_MAPPINGS.keys())\n        # Reading annotation mappings dataframe\n        # Making df of region names and their parent region names\n        annot_df = annot_dict2df(read_json(pfm.map))\n        # Combining (summing) the cells_groagg values for parent regions using the annot_df\n        cells_agg_df = combine_nested_regions(cells_agg_df, annot_df)\n        # Calculating integrated average intensity (sum_intensity / volume)\n        cells_agg_df[CellColumns.IOV.value] = (\n            cells_agg_df[CellColumns.SUM_INTENSITY.value]\n            / cells_agg_df[CellColumns.VOLUME.value]\n        )\n        # Selecting and ordering relevant columns\n        cells_agg_df = cells_agg_df[[*ANNOT_COLUMNS_FINAL, *enum2list(CellColumns)]]\n        # Saving to disk\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_agg = dd.from_pandas(cells_agg)\n        cells_agg_df.to_parquet(pfm.cells_agg_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.make_mask_pipeline","title":"<code>make_mask_pipeline(pfm, overwrite=False)</code>","text":"<p>Makes mask of actual image in reference space. Also stores # and proportion of existent voxels for each region.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef make_mask_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Makes mask of actual image in reference space.\n    Also stores # and proportion of existent voxels\n    for each region.\n    \"\"\"\n    # Getting configs\n    configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n    # Reading ref and trimmed imgs\n    ref_arr = tifffile.imread(pfm.ref)\n    trimmed_arr = tifffile.imread(pfm.trimmed)\n    # Making mask\n    blur_arr = Gf.gauss_blur_filt(trimmed_arr, configs.mask_gaus_blur)\n    tifffile.imwrite(pfm.premask_blur, blur_arr)\n    mask_arr = Gf.manual_thresh(blur_arr, configs.mask_thresh)\n    tifffile.imwrite(pfm.mask, mask_arr)\n\n    # Make outline\n    outline_df = make_outline(mask_arr)\n    # Transformix on coords\n    outline_df[[Coords.Z.value, Coords.Y.value, Coords.X.value]] = (\n        transformation_coords(\n            outline_df,\n            pfm.ref,\n            pfm.regresult,\n        )[[Coords.Z.value, Coords.Y.value, Coords.X.value]]\n        .round(0)\n        .astype(np.int32)\n    )\n    # Filtering out of bounds coords\n    s = ref_arr.shape\n    outline_df = outline_df.query(\n        f\"\"\"\n        ({Coords.Z.value} &gt;= 0) &amp; ({Coords.Z.value} &lt; {s[0]}) &amp;\n        ({Coords.Y.value} &gt;= 0) &amp; ({Coords.Y.value} &lt; {s[1]}) &amp;\n        ({Coords.X.value} &gt;= 0) &amp; ({Coords.X.value} &lt; {s[2]})\n        \"\"\"\n    )\n\n    # Make outline img (1 for in, 2 for out)\n    # TODO: convert to return np.array and save out-of-function\n    coords2points(\n        outline_df[outline_df.is_in == 1],\n        ref_arr.shape,\n        pfm.outline,\n    )\n    in_arr = tifffile.imread(pfm.outline)\n    coords2points(\n        outline_df[outline_df.is_in == 0],\n        ref_arr.shape,\n        pfm.outline,\n    )\n    out_arr = tifffile.imread(pfm.outline)\n    tifffile.imwrite(pfm.outline, in_arr + out_arr * 2)\n\n    # Fill in outline to recreate mask (not perfect)\n    mask_reg_arr = fill_outline(outline_df, ref_arr.shape)\n    # Opening (removes FP) and closing (fills FN)\n    mask_reg_arr = ndimage.binary_closing(mask_reg_arr, iterations=2).astype(np.uint8)\n    mask_reg_arr = ndimage.binary_opening(mask_reg_arr, iterations=2).astype(np.uint8)\n    # Saving\n    tifffile.imwrite(pfm.mask_reg, mask_reg_arr)\n\n    # Counting mask voxels in each region\n    annot_arr = tifffile.imread(pfm.annot)\n    annot_df = annot_dict2df(read_json(pfm.map))\n    # Getting the annotation name for every cell (zyx coord)\n    mask_df = pd.merge(\n        left=mask2region_counts(np.full(annot_arr.shape, 1), annot_arr),\n        right=mask2region_counts(mask_reg_arr, annot_arr),\n        how=\"left\",\n        left_index=True,\n        right_index=True,\n        suffixes=(\"_annot\", \"_mask\"),\n    ).fillna(0)\n    # Combining (summing) the cells_agg_df values for parent regions using the annot_df\n    mask_df = combine_nested_regions(mask_df, annot_df)\n    # Calculating proportion of mask volume in each region\n    mask_df[MaskColumns.VOLUME_PROP.value] = (\n        mask_df[MaskColumns.VOLUME_MASK.value] / mask_df[MaskColumns.VOLUME_ANNOT.value]\n    )\n    # Selecting and ordering relevant columns\n    mask_df = mask_df[[*ANNOT_COLUMNS_FINAL, *enum2list(MaskColumns)]]\n    # Saving\n    mask_df.to_parquet(pfm.mask_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.overwrite_check_decorator","title":"<code>overwrite_check_decorator(func)</code>","text":"<p>Decorator to check overwrite and will not run the function if the output file (as specified in <code>overwrite_fp_map</code>) already exists.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>def overwrite_check_decorator(func: Callable):\n    \"\"\"\n    Decorator to check overwrite and will\n    not run the function if the output file\n    (as specified in `overwrite_fp_map`) already exists.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Getting overwrite arg\n        overwrite = kwargs.get(\"overwrite\", False)\n        # If overwrite is False, check if output file exists\n        if not overwrite:\n            # Getting pfm arg\n            pfm = kwargs.get(\"pfm\", args[0])\n            # Iterating through filepaths that will be overwritten\n            # No checks if func name not in overwrite_fp_map\n            for fp in overwrite_fp_map.get(func.__name__, []):\n                if os.path.exists(getattr(pfm, fp)):\n                    logging.info(f\"Skipping {func.__name__} as {fp} already exists.\")\n                    return\n        # Running func\n        return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.tiff2zarr_pipeline","title":"<code>tiff2zarr_pipeline(pfm, in_fp, overwrite=False)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>pfm</code> <code>ProjFpModel</code> <p>description</p> required <code>in_fp</code> <code>str</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>description, by default False</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef tiff2zarr_pipeline(\n    pfm: ProjFpModel,\n    in_fp: str,\n    overwrite: bool = False,\n):\n    \"\"\"\n    _summary_\n\n    Parameters\n    ----------\n    pfm : ProjFpModel\n        _description_\n    in_fp : str\n        _description_\n    overwrite : bool, optional\n        _description_, by default False\n\n    Raises\n    ------\n    ValueError\n        _description_\n    \"\"\"\n    # Getting configs\n    configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n    # Making zarr from tiff file(s)\n    with cluster_proc_contxt(LocalCluster(n_workers=1, threads_per_worker=6)):\n        if os.path.isdir(in_fp):\n            # If in_fp is a directory, make zarr from the tiff file stack in directory\n            tiffs2zarr(\n                in_fp_ls=natsorted(\n                    [\n                        os.path.join(in_fp, i)\n                        for i in os.listdir(in_fp)\n                        if re.search(r\".tif$\", i)\n                    ]\n                ),\n                out_fp=pfm.raw,\n                chunks=configs.chunksize,\n            )\n        elif os.path.isfile(in_fp):\n            # If in_fp is a file, make zarr from the btiff file\n            btiff2zarr(\n                in_fp=in_fp,\n                out_fp=pfm.raw,\n                chunks=configs.chunksize,\n            )\n        else:\n            raise ValueError(f'Input file path, \"{in_fp}\" does not exist.')\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.transform_coords_pipeline","title":"<code>transform_coords_pipeline(pfm, overwrite=False)</code>","text":"<p><code>in_id</code> and <code>out_id</code> are either maxima or region</p> <p>NOTE: saves the cells_trfm dataframe as pandas parquet.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef transform_coords_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    `in_id` and `out_id` are either maxima or region\n\n    NOTE: saves the cells_trfm dataframe as pandas parquet.\n    \"\"\"\n    # Getting configs\n    configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n    with cluster_proc_contxt(LocalCluster(n_workers=4, threads_per_worker=1)):\n        # Setting output key (in the form \"&lt;maxima/region&gt;_trfm_df\")\n        # Getting cell coords\n        cells_df = dd.read_parquet(pfm.cells_raw_df).compute()\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        # Taking only Coords.Z.value, Coords.Y.value, Coords.X.value coord columns\n        cells_df = cells_df[[Coords.Z.value, Coords.Y.value, Coords.X.value]]\n        # Scaling to resampled rough space\n        # NOTE: this downsampling uses slicing so must be computed differently\n        cells_df = cells_df / np.array(\n            (configs.z_rough, configs.y_rough, configs.x_rough)\n        )\n        # Scaling to resampled space\n        cells_df = cells_df * np.array((configs.z_fine, configs.y_fine, configs.x_fine))\n        # Trimming/offsetting to sliced space\n        cells_df = cells_df - np.array(\n            [\n                s[0] if s[0] else 0\n                for s in (configs.z_trim, configs.y_trim, configs.x_trim)\n            ]\n        )\n\n        cells_trfm_df = transformation_coords(cells_df, pfm.ref, pfm.regresult)\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_df = dd.from_pandas(cells_df, npartitions=1)\n        # Fitting resampled space to atlas image with Transformix (from Elastix registration step)\n        # cells_df = cells_df.repartition(\n        #     npartitions=int(np.ceil(cells_df.shape[0].compute() / ROWSPPART))\n        # )\n        # cells_df = cells_df.map_partitions(\n        #     transformation_coords, pfm.ref, pfm.regresult\n        # )\n        cells_trfm_df.to_parquet(pfm.cells_trfm_df)\n</code></pre>"},{"location":"reference/proj_fp_model.html","title":"Proj fp model","text":""},{"location":"reference/proj_fp_model.html#microscopy_proc.utils.proj_org_utils.ProjFpModel","title":"<code>microscopy_proc.utils.proj_org_utils.ProjFpModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for project file paths.</p> Source code in <code>microscopy_proc/utils/proj_org_utils.py</code> <pre><code>class ProjFpModel(BaseModel):\n    \"\"\"\n    Pydantic model for project file paths.\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n        arbitrary_types_allowed=True,\n        validate_default=True,\n        use_enum_values=True,\n    )\n\n    # ROOT DIR\n    root_dir: str\n    # CONFIGS\n    config_params: str\n    # ATLAS AND ELASTIX PARAMS FILES\n    ref: str\n    annot: str\n    map: str\n    affine: str\n    bspline: str\n    # RAW IMG FILE\n    raw: str\n    # REGISTRATION PROCESSING FILES\n    downsmpl1: str\n    downsmpl2: str\n    trimmed: str\n    regresult: str\n    # WHOLE MASK\n    premask_blur: str\n    mask: str\n    outline: str\n    mask_reg: str\n    mask_df: str\n    # CELL COUNTING ARRAY FILES\n    overlap: str\n    bgrm: str\n    dog: str\n    adaptv: str\n    threshd: str\n    threshd_volumes: str\n    threshd_filt: str\n    maxima: str\n    wshed_volumes: str\n    wshed_filt: str\n    threshd_final: str\n    maxima_final: str\n    wshed_final: str\n    # CELL COUNTING DF FILES\n    maxima_df: str\n    cells_raw_df: str\n    cells_trfm_df: str\n    cells_df: str\n    cells_agg_df: str\n    cells_agg_csv: str\n    # VISUAL CHECK FROM CELL DF FILES\n    points_check: str\n    heatmap_check: str\n    points_trfm_check: str\n    heatmap_trfm_check: str\n\n    @classmethod\n    def get_proj_fp_model(cls, proj_dir):\n        return cls(\n            # ROOT DIR\n            root_dir=proj_dir,\n            # CONFIGS\n            config_params=os.path.join(proj_dir, \"config_params.json\"),\n            # MY ATLAS AND ELASTIX PARAMS FILES\n            ref=os.path.join(proj_dir, \"registration\", \"0a_reference.tif\"),\n            annot=os.path.join(proj_dir, \"registration\", \"0b_annotation.tif\"),\n            map=os.path.join(proj_dir, \"registration\", \"0c_mapping.json\"),\n            affine=os.path.join(proj_dir, \"registration\", \"0d_align_affine.txt\"),\n            bspline=os.path.join(proj_dir, \"registration\", \"0e_align_bspline.txt\"),\n            # RAW IMG FILE\n            raw=os.path.join(proj_dir, \"raw.zarr\"),\n            # REGISTRATION PROCESSING FILES\n            downsmpl1=os.path.join(proj_dir, \"registration\", \"1_downsmpl1.tif\"),\n            downsmpl2=os.path.join(proj_dir, \"registration\", \"2_downsmpl2.tif\"),\n            trimmed=os.path.join(proj_dir, \"registration\", \"3_trimmed.tif\"),\n            regresult=os.path.join(proj_dir, \"registration\", \"4_regresult.tif\"),\n            # WHOLE MASK\n            premask_blur=os.path.join(proj_dir, \"mask\", \"1_premask_blur.tif\"),\n            mask=os.path.join(proj_dir, \"mask\", \"2_mask_trimmed.tif\"),\n            outline=os.path.join(proj_dir, \"mask\", \"3_outline_reg.tif\"),\n            mask_reg=os.path.join(proj_dir, \"mask\", \"4_mask_reg.tif\"),\n            mask_df=os.path.join(proj_dir, \"mask\", \"5_mask.parquet\"),\n            # CELL COUNTING ARRAY FILES\n            overlap=os.path.join(proj_dir, \"cellcount\", \"0_overlap.zarr\"),\n            bgrm=os.path.join(proj_dir, \"cellcount\", \"1_bgrm.zarr\"),\n            dog=os.path.join(proj_dir, \"cellcount\", \"2_dog.zarr\"),\n            adaptv=os.path.join(proj_dir, \"cellcount\", \"3_adaptv.zarr\"),\n            threshd=os.path.join(proj_dir, \"cellcount\", \"4_threshd.zarr\"),\n            threshd_volumes=os.path.join(\n                proj_dir, \"cellcount\", \"5_threshd_volumes.zarr\"\n            ),\n            threshd_filt=os.path.join(proj_dir, \"cellcount\", \"6_threshd_filt.zarr\"),\n            maxima=os.path.join(proj_dir, \"cellcount\", \"7_maxima.zarr\"),\n            wshed_volumes=os.path.join(proj_dir, \"cellcount\", \"8_wshed_volumes.zarr\"),\n            wshed_filt=os.path.join(proj_dir, \"cellcount\", \"9_wshed_filt.zarr\"),\n            # CELL COUNTING TRIMMED ARRAY FILES\n            threshd_final=os.path.join(proj_dir, \"cellcount\", \"10_threshd_f.zarr\"),\n            maxima_final=os.path.join(proj_dir, \"cellcount\", \"10_maxima_f.zarr\"),\n            wshed_final=os.path.join(proj_dir, \"cellcount\", \"10_wshed_f.zarr\"),\n            # CELL COUNTING DF FILES\n            maxima_df=os.path.join(proj_dir, \"analysis\", \"11_maxima.parquet\"),\n            cells_raw_df=os.path.join(proj_dir, \"analysis\", \"11_cells_raw.parquet\"),\n            cells_trfm_df=os.path.join(proj_dir, \"analysis\", \"12_cells_trfm.parquet\"),\n            cells_df=os.path.join(proj_dir, \"analysis\", \"13_cells.parquet\"),\n            cells_agg_df=os.path.join(proj_dir, \"analysis\", \"14_cells_agg.parquet\"),\n            cells_agg_csv=os.path.join(proj_dir, \"analysis\", \"15_cells_agg.csv\"),\n            # VISUAL CHECK FROM CELL DF FILES\n            points_check=os.path.join(proj_dir, \"visual_check\", \"points.zarr\"),\n            heatmap_check=os.path.join(proj_dir, \"visual_check\", \"heatmap.zarr\"),\n            points_trfm_check=os.path.join(\n                proj_dir, \"visual_check\", \"points_trfm.zarr\"\n            ),\n            heatmap_trfm_check=os.path.join(\n                proj_dir, \"visual_check\", \"heatmap_trfm.zarr\"\n            ),\n        )\n</code></pre>"},{"location":"tutorials/pipeline.html","title":"Pipeline","text":""},{"location":"tutorials/tutorial.html","title":"Setup","text":"<p>Before running the behavysis_pipeline analysises, the files that we want to analyse must be set up a certain way for the behavysis_pipeline program to recognise them.</p> <p>There are three important guidelines to set up the project:</p> <ul> <li>Structure of files in folders .</li> <li>Experiment files.</li> <li>Config files for each experiment.</li> </ul>"},{"location":"tutorials/tutorial.html#folder-structure","title":"Folder Structure","text":"<p>They need to be set up inside specially named folders, as shown below.</p> <p>An example of how this would look on a computer (in this case, a Mac) is shown below.</p>"},{"location":"tutorials/tutorial.html#experiment-files","title":"Experiment Files","text":"<p>Each experiment must have files that have same name (not including the suffix like <code>.csv</code> or <code>.mp4</code>). An example is \"day1_experiment1\" must have all files named \"day1_experiment1.mp4\", \"day1_experiment1.csv\", \"day1_experiment1.json\" etc. stored in the corresponding folder.</p>"},{"location":"tutorials/tutorial.html#config-files","title":"Config Files","text":"<p>The config file for an experiment stores all the parameters for how the experiment was recorded (e.g., the frames per second of the raw video, the experiment duration, etc.), and the parameters for how we want to process the data (e.g., the intended frames per second to format the video to, the DLC model to use to analyse, the likeliness pcutoff to interpolate points, etc.)</p> <p>An example of a config file is shown here.</p>"},{"location":"tutorials/tutorial.html#running-behavysis_pipeline","title":"Running behavysis_pipeline","text":"<p>To install <code>behavysis_pipeline</code>, follow these instructions.</p> <p>To run <code>behavysis_pipeline</code>, follow these these instructions.</p>"}]}