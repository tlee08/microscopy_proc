var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>This site contains the documentation for the <code>behavysis_pipeline</code> program.</p>"},{"location":"examples/batch_pipeline.html","title":"Analysing a Folder of Experiments","text":"<p>All outcomes for experiment processing is stored in csv files in the <code>proj_dir/diagnostics</code> folder. These files store the outcome and process description (i.e. error explanations) of all experiments.</p>"},{"location":"examples/batch_pipeline.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>from behavysis_pipeline import Project\nfrom behavysis_pipeline.processes import *\n</code></pre>"},{"location":"examples/batch_pipeline.html#making-the-project-and-importing-all-experiments","title":"Making the project and importing all experiments","text":"<p>The directory path of the project must be specified and must contain the experiment files you wish to analyse in a particular folder structure.</p> <p>For more information on how to structure a project directory, please see setup.</p> <p>For more information on how a <code>Experiment</code> works, please see behavysis_pipeline.pipeline.project.Project.</p> <pre><code># Defining the project's folder\nproj_dir = \"./project\"\n# Initialising the project\nproj = Project(proj_dir)\n# Importing all the experiments (from the project folder)\nproj.importExperiments()\n</code></pre>"},{"location":"examples/batch_pipeline.html#checking-all-imported-experiments","title":"Checking all imported experiments","text":"<p>To see all imported experiments, see the <code>proj_dir/diagnostics/importExperiments.csv</code> file that has been generated.</p>"},{"location":"examples/batch_pipeline.html#updating-the-configurations-for-all-experiments","title":"Updating the configurations for all experiments","text":"<p>If you would like the configurations (which are stored in config files) to be updated new parameters, define the JSON style of configuration parameters you would like to add and run the following lines.</p> <p>For more information about how a configurations file works, please see here.</p> <pre><code># Defining the default configs json path\nconfigs_fp = \"path/to/default_configs.json\"\n# Overwriting the configs\nproj.update_configs(\n    configs_fp,\n    overwrite=\"user\",\n)\n</code></pre>"},{"location":"examples/batch_pipeline.html#get-animal-keypoints-in-videos","title":"Get Animal Keypoints in Videos","text":"<p>The following code processes and analyses all experiments that have been imported into a project. This is similar to analysing a single experiment.</p>"},{"location":"examples/batch_pipeline.html#downsample-videos","title":"Downsample videos","text":"<p>Formatting the raw mp4 videos so it can be fed through the DLC pose estimation algorithm.</p> <pre><code>proj.format_vid(\n    (\n        FormatVid.format_vid,\n        FormatVid.get_vid_metadata,\n    ),\n    overwrite=True,\n)\n</code></pre>"},{"location":"examples/batch_pipeline.html#run-keypoints-detection-deeplabcut","title":"Run Keypoints detection (DeepLabCut)","text":"<p>Running the DLC pose estimation algorithm on the formatted mp4 files.</p> <p>Note</p> <p>Make sure to change the <code>user.run_dlc.model_fp</code> to the DeepLabCut model's config file you'd like to use.</p> <pre><code>proj.run_dlc(\n    gputouse=None,\n    overwrite=True,\n)\n</code></pre>"},{"location":"examples/batch_pipeline.html#calculating-inherent-parameters-from-keypoints","title":"Calculating Inherent Parameters from Keypoints","text":"<p>Calculating relevant parameters to store in the <code>auto</code> section of the config file. The calculations performed are:</p> <pre><code>proj.calculate_params(\n    (\n        CalculateParams.start_frame,\n        CalculateParams.stop_frame,\n        CalculateParams.exp_dur,\n        CalculateParams.px_per_mm,\n    )\n)\n</code></pre> <p>And see a collation of all experiments' inherent parameters to spot any anomolies before continuing</p> <pre><code>proj.collate_configs_auto()\n</code></pre>"},{"location":"examples/batch_pipeline.html#postprocessing","title":"Postprocessing","text":"<p>Preprocessing the DLC csv data and output the preprocessed data to a <code>preprocessed_csv.&lt;exp_name&gt;.csv</code> file. The preprocessings performed are:</p> <pre><code>proj.preprocess(\n    (\n        Preprocess.start_stop_trim,\n        Preprocess.interpolate,\n        Preprocess.refine_ids,\n    ),\n    overwrite=overwrite,\n)\n</code></pre>"},{"location":"examples/batch_pipeline.html#make-simple-analysis","title":"Make Simple Analysis","text":"<p>Analysing the preprocessed csv data to extract useful analysis and results. The analyses performed are:</p> <pre><code>proj.analyse(\n    (\n        Analyse.thigmotaxis,\n        Analyse.center_crossing,\n        Analyse.in_roi,\n        Analyse.speed,\n        Analyse.social_distance,\n        Analyse.freezing,\n    )\n)\n</code></pre>"},{"location":"examples/batch_pipeline.html#automated-behaviour-detection","title":"Automated Behaviour Detection","text":""},{"location":"examples/batch_pipeline.html#extracting-features","title":"Extracting Features","text":"<p>Extracting derivative features from keypoints. For example - speed, bounding ellipse size, distance between points, etc.</p> <pre><code>proj.extract_features(overwrite)\n</code></pre>"},{"location":"examples/batch_pipeline.html#running-behaviour-classifiers","title":"Running Behaviour Classifiers","text":"<p>Note</p> <p>Make sure to change the <code>user.classify_behaviours</code> list to the behaviours classifiers you'd like to use.</p> <pre><code>proj.classify_behaviours(overwrite)\n</code></pre>"},{"location":"examples/batch_pipeline.html#exporting-the-behaviour-detection-results","title":"Exporting the Behaviour Detection Results","text":"<p>Exports to such a format, where a) <code>behavysis_viewer</code> can load it and perform semi-automated analysis, and b) after semi-automated verification, can be used to make a new/improve a current behaviour classifier (with behavysis_classifier.behav_classifier.BehavClassifier)</p> <pre><code>proj.export_behaviours(overwrite)\n</code></pre>"},{"location":"examples/batch_pipeline.html#analyse-behaviours","title":"Analyse Behaviours","text":"<p>Similar to simple analysis, which calculates each experiment's a) the overall summary, and b) binned summary.</p> <pre><code>proj.behav_analyse()\n</code></pre>"},{"location":"examples/batch_pipeline.html#export-any-tables","title":"Export any Tables","text":"<p>Tables are stored as <code>.feather</code> files.</p> <p>To export these to csv files, run the following:</p> <pre><code>proj.export_feather(\"7_scored_behavs\", \"/path/to/csv_out\")\n</code></pre>"},{"location":"examples/batch_pipeline.html#evaluate","title":"Evaluate","text":"<p>Evaluates keypoints and behaviours accuracy by making annotated experiment videos.</p> <pre><code>proj.evaluate(\n    (\n        Evaluate.eval_vid,\n        Evaluate.keypoints_plot,\n    ),\n    overwrite=overwrite,\n)\n</code></pre>"},{"location":"examples/single_pipeline.html","title":"Training a Behaviour Classifier","text":""},{"location":"examples/single_pipeline.html#loading-in-all-relevant-packages","title":"Loading in all relevant packages","text":"<pre><code>import os\n\nfrom behavysis_core.mixins.behav_mixin import BehavMixin\nfrom behavysis_classifier import BehavClassifier\nfrom behavysis_classifier.clf_models.clf_templates import ClfTemplates\nfrom behavysis_pipeline.pipeline import Project\n\nif __name__ == \"__main__\":\n    root_dir = \".\"\n    overwrite = True\n\n    # Option 1: From BORIS\n    # Define behaviours in BORIS\n    behavs_ls = [\"potential huddling\", \"huddling\"]\n    # Paths\n    configs_dir = os.path.join(root_dir, \"0_configs\")\n    boris_dir = os.path.join(root_dir, \"boris\")\n    out_dir = os.path.join(root_dir, \"7_scored_behavs\")\n    # Getting names of all files\n    names = [os.path.splitext(i)[0] for i in os.listdir(boris_dir)]\n    for name in names:\n        # Paths\n        boris_fp = os.path.join(boris_dir, f\"{name}.tsv\")\n        configs_fp = os.path.join(configs_dir, f\"{name}.json\")\n        out_fp = os.path.join(out_dir, f\"{name}.feather\")\n        # Making df from BORIS\n        df = BehavMixin.import_boris_tsv(boris_fp, configs_fp, behavs_ls)\n        # Saving df\n        df.to_feather(out_fp)\n    # Making BehavClassifier objects\n    for behav in behavs_ls:\n        BehavClassifier.create_new_model(os.path.join(root_dir, \"behav_models\"), behav)\n\n    # Option 2: From previous behavysis project\n    proj = Project(root_dir)\n    proj.import_experiments()\n    # Making BehavClassifier objects\n    BehavClassifier.create_from_project(proj)\n\n    # Loading a BehavModel\n    behav = \"fight\"\n    model = BehavClassifier.load(\n        os.path.join(root_dir, \"behav_models\", f\"{behav}.json\")\n    )\n    # Testing all different classifiers\n    model.clf_eval_compare_all()\n    # MANUALLY LOOK AT THE BEST CLASSIFIER AND SELECT\n    # Example\n    model.pipeline_build(ClfTemplates.dnn_1)\n</code></pre>"},{"location":"installation/installing.html","title":"Installing","text":"<p>Step 1:</p> <p>Install conda by visiting the Miniconda downloads page and following the prompts to install on your system.</p> <p>Open the downloaded miniconda file and follow the installation prompts.</p> <p>Step 2:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows) and verify that conda has been installed with the following command.</p> <pre><code>conda --version\n</code></pre> <p>A response like <code>conda xx.xx.xx</code> indicates that it has been correctly installed.</p> <p>Step 3:</p> <p>Update conda and use the libmamba solver (makes downloading conda programs MUCH faster):</p> <pre><code>conda update -n base conda\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre> <p>Step 4:</p> <p>Install packages that help Jupyter notebooks read conda environments:</p> <pre><code>conda install -n base nb_conda nb_conda_kernels\n</code></pre> <p>Step 5:</p> <p>Install the <code>behavysis_pipeline</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/conda_env.yaml\n</code></pre> <p>Step 6:</p> <p>Install the <code>DEEPLABCUT</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/DEEPLABCUT.yaml\n</code></pre> <p>Step 7:</p> <p>Install the <code>simba</code> conda environment (download here).</p> <pre><code>conda env create -f path/to/simba_env.yaml\n</code></pre>"},{"location":"installation/running.html","title":"Running","text":"<p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>Activate the program environment with the following command:</p> <pre><code>conda activate behavysis_pipeline_env\n</code></pre> <p>Step 3:</p> <p>You can now use the <code>behavysis</code> package in a Jupyter kernel or regular Python script.</p> <p>See here for examples of Jupyter notebooks to run behaviour analysis.</p> <p>See here for examples of Jupyter notebooks to train behaviour classifiers.</p> <p>See here for a tutorial of <code>behavysis</code>'s workflow.</p> <p>See here for API documentation.</p> <p>Note</p> <p>To run jupyter, run the following command in the terminal</p> <pre><code>jupyter-lab\n</code></pre> <p>This will open a browser to <code>http://127.0.0.1:8888/lab</code>, where you can run jupyter notebooks.</p> <p>You can also run jupyter notebooks in VS Code.</p>"},{"location":"installation/uninstalling.html","title":"Uninstalling","text":"<p>For more information about how to uninstall conda, see here.</p> <p>Step 1:</p> <p>Open a terminal (Mac or Linux) or Anaconda PowerShell Prompt (Windows)</p> <p>Step 2:</p> <p>To uninstall the <code>behavysis_pipeline_env</code>, <code>DEEPLABCUT</code>, and <code>simba</code> conda envs, run the following commands:</p> <pre><code>conda env remove -n behavysis_pipeline_env\nconda env remove -n DEEPLABCUT\nconda env remove -n simba\n</code></pre> <p>Step 3:</p> <p>To remove conda, enter the following commands in the terminal.</p> <pre><code>conda install anaconda-clean\nanaconda-clean --yes\n\nrm -rf ~/anaconda3\nrm -rf ~/opt/anaconda3\nrm -rf ~/.anaconda_backup\n</code></pre> <p>Step 5: Edit your bash or zsh profile so conda it does not look for conda anymore. Open each of these files (note that not all of them may exist on your computer), <code>~/.zshrc</code>, <code>~/.zprofile</code>, or <code>~/.bash_profile</code>, with the following command.</p> <pre><code>open ~/.zshrc\nopen ~/.zprofile\nopen ~/.bash_profile\n</code></pre>"},{"location":"installation/updating.html","title":"Updating","text":"<p>Step 1: Download the <code>conda_env.yaml</code> file from here</p> <p>Step 2: Run the following command to update <code>behavysis_pipeline</code>:</p> <pre><code>conda env update -f conda_env.yaml --prune\n</code></pre>"},{"location":"reference/pipeline_funcs.html","title":"Pipeline funcs","text":""},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs","title":"<code>microscopy_proc.pipelines.pipeline_funcs</code>","text":""},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.all_pipeline","title":"<code>all_pipeline(in_fp, pfm, overwrite=False)</code>","text":"<p>Running all pipelines in order.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>def all_pipeline(\n    in_fp: str,\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Running all pipelines in order.\n    \"\"\"\n    # Running all pipelines in order\n    tiff2zarr_pipeline(pfm, in_fp, overwrite=overwrite)\n    ref_prepare_pipeline(pfm, overwrite=overwrite)\n    img_rough_pipeline(pfm, overwrite=overwrite)\n    img_fine_pipeline(pfm, overwrite=overwrite)\n    img_trim_pipeline(pfm, overwrite=overwrite)\n    registration_pipeline(pfm, overwrite=overwrite)\n    make_mask_pipeline(pfm, overwrite=overwrite)\n    img_overlap_pipeline(pfm, overwrite=overwrite)\n    cellc1_pipeline(pfm, overwrite=overwrite)\n    cellc2_pipeline(pfm, overwrite=overwrite)\n    cellc3_pipeline(pfm, overwrite=overwrite)\n    cellc4_pipeline(pfm, overwrite=overwrite)\n    cellc5_pipeline(pfm, overwrite=overwrite)\n    cellc6_pipeline(pfm, overwrite=overwrite)\n    cellc7_pipeline(pfm, overwrite=overwrite)\n    cellc8_pipeline(pfm, overwrite=overwrite)\n    cellc9_pipeline(pfm, overwrite=overwrite)\n    cellc10_pipeline(pfm, overwrite=overwrite)\n    cellc11_pipeline(pfm, overwrite=overwrite)\n    cellc_coords_only_pipeline(pfm, overwrite=overwrite)\n    transform_coords_pipeline(pfm, overwrite=overwrite)\n    cell_mapping_pipeline(pfm, overwrite=overwrite)\n    group_cells_pipeline(pfm, overwrite=overwrite)\n    cells2csv_pipeline(pfm, overwrite=overwrite)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cell_mapping_pipeline","title":"<code>cell_mapping_pipeline(pfm, overwrite=False)</code>","text":"<p>Using the transformed cell coordinates, get the region ID and name for each cell corresponding to the reference atlas.</p> <p>NOTE: saves the cells dataframe as pandas parquet.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cell_mapping_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Using the transformed cell coordinates, get the region ID and name for each cell\n    corresponding to the reference atlas.\n\n    NOTE: saves the cells dataframe as pandas parquet.\n    \"\"\"\n    # Getting region for each detected cell (i.e. row) in cells_df\n    with cluster_proc_contxt(LocalCluster()):\n        # Reading cells_raw and cells_trfm dataframes\n        cells_df = dd.read_parquet(pfm.cells_raw_df).compute()\n        coords_trfm = pd.read_parquet(pfm.cells_trfm_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        coords_trfm = sanitise_smb_df(coords_trfm)\n        # Making unique incrementing index\n        cells_df = cells_df.reset_index(drop=True)\n        # Setting the transformed coords\n        cells_df[f\"{Coords.Z.value}_{TRFM}\"] = coords_trfm[Coords.Z.value].values\n        cells_df[f\"{Coords.Y.value}_{TRFM}\"] = coords_trfm[Coords.Y.value].values\n        cells_df[f\"{Coords.X.value}_{TRFM}\"] = coords_trfm[Coords.X.value].values\n\n        # Reading annotation image\n        annot_arr = tifffile.imread(pfm.annot)\n        # Getting the annotation ID for every cell (zyx coord)\n        # Getting transformed coords (that are within tbe bounds_arr, and their corresponding idx)\n        s = annot_arr.shape\n        trfm_loc = (\n            cells_df[\n                [\n                    f\"{Coords.Z.value}_{TRFM}\",\n                    f\"{Coords.Y.value}_{TRFM}\",\n                    f\"{Coords.X.value}_{TRFM}\",\n                ]\n            ]\n            .round(0)\n            .astype(np.int32)\n            .query(\n                f\"\"\"\n                ({Coords.Z.value}_{TRFM} &gt;= 0) &amp; ({Coords.Z.value}_{TRFM} &lt; {s[0]}) &amp;\n                ({Coords.Y.value}_{TRFM} &gt;= 0) &amp; ({Coords.Y.value}_{TRFM} &lt; {s[1]}) &amp;\n                ({Coords.X.value}_{TRFM} &gt;= 0) &amp; ({Coords.X.value}_{TRFM} &lt; {s[2]})\n                \"\"\"\n            )\n        )\n        # Getting the pixel values of each valid transformed coord (hence the specified index)\n        # By complex array indexing on ar_annot's (z, y, x) dimensions.\n        # nulls are imputed with -1\n        cells_df[AnnotColumns.ID.value] = pd.Series(\n            annot_arr[*trfm_loc.values.T].astype(np.uint32),\n            index=trfm_loc.index,\n        ).fillna(-1)\n\n        # Reading annotation mappings dataframe\n        annot_df = annot_dict2df(read_json(pfm.map))\n        # Getting the annotation name for every cell (zyx coord)\n        cells_df = df_map_ids(cells_df, annot_df)\n        # Saving to disk\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_df = dd.from_pandas(cells_df)\n        cells_df.to_parquet(pfm.cells_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc10_pipeline","title":"<code>cellc10_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 10</p> <p>Trimming filtered regions overlaps to make: - Trimmed maxima image - Trimmed threshold image - Trimmed watershed image</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc10_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 10\n\n    Trimming filtered regions overlaps to make:\n    - Trimmed maxima image\n    - Trimmed threshold image\n    - Trimmed watershed image\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n        # Declaring processing instructions\n        maxima_final_arr = da_trim(maxima_arr, d=configs.depth)\n        threshd_final_arr = da_trim(threshd_filt_arr, d=configs.depth)\n        wshed_final_arr = da_trim(wshed_volumes_arr, d=configs.depth)\n        # Computing and saving\n        disk_cache(maxima_final_arr, pfm.maxima_final)\n        disk_cache(threshd_final_arr, pfm.threshd_final)\n        disk_cache(wshed_final_arr, pfm.wshed_final)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc11_pipeline","title":"<code>cellc11_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 11</p> <p>From maxima and watershed, save the cells.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc11_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 11\n\n    From maxima and watershed, save the cells.\n    \"\"\"\n    with cluster_proc_contxt(LocalCluster(n_workers=2, threads_per_worker=1)):\n        # n_workers=2\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        raw_arr = da.from_zarr(pfm.raw)\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_arr = da.from_zarr(pfm.maxima)\n        wshed_filt_arr = da.from_zarr(pfm.wshed_filt)\n        # Declaring processing instructions\n        # Getting maxima coords and cell measures in table\n        cells_df = block2coords(\n            Cf.get_cells,\n            raw_arr,\n            overlap_arr,\n            maxima_arr,\n            wshed_filt_arr,\n            configs.depth,\n        )\n        # Filtering out by volume (same filter cellc9_pipeline volume_filter)\n        cells_df = cells_df.query(\n            f\"\"\"\n            ({CellColumns.VOLUME.value} &gt;= {configs.min_wshed}) &amp;\n            ({CellColumns.VOLUME.value} &lt;= {configs.max_wshed})\n            \"\"\"\n        )\n        # Computing and saving as parquet\n        cells_df.to_parquet(pfm.cells_raw_df, overwrite=True)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc1_pipeline","title":"<code>cellc1_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 1</p> <p>Top-hat filter (background subtraction)</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc1_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 1\n\n    Top-hat filter (background subtraction)\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        # Declaring processing instructions\n        bgrm_arr = da.map_blocks(\n            Gf.tophat_filt,\n            overlap_arr,\n            configs.tophat_sigma,\n        )\n        # Computing and saving\n        bgrm_arr = disk_cache(bgrm_arr, pfm.bgrm)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc2_pipeline","title":"<code>cellc2_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 2</p> <p>Difference of Gaussians (edge detection)</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc2_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 2\n\n    Difference of Gaussians (edge detection)\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        bgrm_arr = da.from_zarr(pfm.bgrm)\n        # Declaring processing instructions\n        dog_arr = da.map_blocks(\n            Gf.dog_filt,\n            bgrm_arr,\n            configs.dog_sigma1,\n            configs.dog_sigma2,\n        )\n        # Computing and saving\n        dog_arr = disk_cache(dog_arr, pfm.dog)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc3_pipeline","title":"<code>cellc3_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 3</p> <p>Gaussian subtraction with large sigma for adaptive thresholding</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc3_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 3\n\n    Gaussian subtraction with large sigma for adaptive thresholding\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        dog_arr = da.from_zarr(pfm.dog)\n        # Declaring processing instructions\n        adaptv_arr = da.map_blocks(\n            Gf.gauss_subt_filt,\n            dog_arr,\n            configs.gauss_sigma,\n        )\n        # Computing and saving\n        adaptv_arr = disk_cache(adaptv_arr, pfm.adaptv)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc4_pipeline","title":"<code>cellc4_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 4</p> <p>Mean thresholding with standard deviation offset</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc4_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 4\n\n    Mean thresholding with standard deviation offset\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # # Visually inspect sd offset\n        # t_p =adaptv_arr.sum() / (np.prod(adaptv_arr.shape) - (adaptv_arr == 0).sum())\n        # t_p = t_p.compute()\n        # logging.debug(t_p)\n        # Reading input images\n        adaptv_arr = da.from_zarr(pfm.adaptv)\n        # Declaring processing instructions\n        threshd_arr = da.map_blocks(\n            Cf.manual_thresh,\n            adaptv_arr,\n            configs.thresh_p,\n        )\n        # Computing and saving\n        threshd_arr = disk_cache(threshd_arr, pfm.threshd)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc5_pipeline","title":"<code>cellc5_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 5</p> <p>Getting object sizes</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc5_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 5\n\n    Getting object sizes\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster(n_workers=6, threads_per_worker=1)):\n        # Reading input images\n        threshd_arr = da.from_zarr(pfm.threshd)\n        # Declaring processing instructions\n        threshd_volumes_arr = da.map_blocks(\n            Cf.label_with_volumes,\n            threshd_arr,\n        )\n        # Computing and saving\n        threshd_volumes_arr = disk_cache(threshd_volumes_arr, pfm.threshd_volumes)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc6_pipeline","title":"<code>cellc6_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 6</p> <p>Filter out large objects (likely outlines, not cells)</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc6_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 6\n\n    Filter out large objects (likely outlines, not cells)\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        threshd_volumes_arr = da.from_zarr(pfm.threshd_volumes)\n        # Declaring processing instructions\n        threshd_filt_arr = da.map_blocks(\n            Cf.volume_filter,\n            threshd_volumes_arr,\n            configs.min_threshd,\n            configs.max_threshd,\n        )\n        # Computing and saving\n        threshd_filt_arr = disk_cache(threshd_filt_arr, pfm.threshd_filt)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc7_pipeline","title":"<code>cellc7_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 7</p> <p>Get maxima of image masked by labels.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc7_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 7\n\n    Get maxima of image masked by labels.\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCUDACluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        maxima_arr = da.map_blocks(\n            Gf.get_local_maxima,\n            overlap_arr,\n            configs.maxima_sigma,\n            threshd_filt_arr,\n        )\n        # Computing and saving\n        maxima_arr = disk_cache(maxima_arr, pfm.maxima)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc8_pipeline","title":"<code>cellc8_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 8</p> <p>Watershed segmentation volumes.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc8_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 8\n\n    Watershed segmentation volumes.\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster(n_workers=3, threads_per_worker=1)):\n        # n_workers=2\n        # Reading input images\n        overlap_arr = da.from_zarr(pfm.overlap)\n        maxima_arr = da.from_zarr(pfm.maxima)\n        threshd_filt_arr = da.from_zarr(pfm.threshd_filt)\n        # Declaring processing instructions\n        wshed_volumes_arr = da.map_blocks(\n            Cf.wshed_segm_volumes,\n            overlap_arr,\n            maxima_arr,\n            threshd_filt_arr,\n        )\n        # Computing and saving\n        wshed_volumes_arr = disk_cache(wshed_volumes_arr, pfm.wshed_volumes)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc9_pipeline","title":"<code>cellc9_pipeline(pfm, overwrite=False)</code>","text":"<p>Cell counting pipeline - Step 9</p> <p>Filter out large watershed objects (again likely outlines, not cells).</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc9_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Cell counting pipeline - Step 9\n\n    Filter out large watershed objects (again likely outlines, not cells).\n    \"\"\"\n    # Making Dask cluster\n    with cluster_proc_contxt(LocalCluster()):\n        # Getting configs\n        configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n        # Reading input images\n        wshed_volumes_arr = da.from_zarr(pfm.wshed_volumes)\n        # Declaring processing instructions\n        wshed_filt_arr = da.map_blocks(\n            Cf.volume_filter,\n            wshed_volumes_arr,\n            configs.min_wshed,\n            configs.max_wshed,\n        )\n        # Computing and saving\n        wshed_filt_arr = disk_cache(wshed_filt_arr, pfm.wshed_filt)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.cellc_coords_only_pipeline","title":"<code>cellc_coords_only_pipeline(pfm, overwrite=False)</code>","text":"<p>Get maxima coords. Very basic but faster version of cellc11_pipeline get_cells.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef cellc_coords_only_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Get maxima coords.\n    Very basic but faster version of cellc11_pipeline get_cells.\n    \"\"\"\n    # Reading filtered and maxima images (trimmed - orig space)\n    with cluster_proc_contxt(LocalCluster(n_workers=6, threads_per_worker=1)):\n        # Read filtered and maxima images (trimmed - orig space)\n        maxima_final_arr = da.from_zarr(pfm.maxima_final)\n        # Declaring processing instructions\n        # Storing coords of each maxima in df\n        coords_df = block2coords(\n            Gf.get_coords,\n            maxima_final_arr,\n        )\n        # Computing and saving as parquet\n        coords_df.to_parquet(pfm.maxima_df, overwrite=True)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.group_cells_pipeline","title":"<code>group_cells_pipeline(pfm, overwrite=False)</code>","text":"<p>Grouping cells by region name and aggregating total cell volume and cell count for each region.</p> <p>NOTE: saves the cells_agg dataframe as pandas parquet.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef group_cells_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Grouping cells by region name and aggregating total cell volume\n    and cell count for each region.\n\n    NOTE: saves the cells_agg dataframe as pandas parquet.\n    \"\"\"\n    # Making cells_agg_df\n    with cluster_proc_contxt(LocalCluster()):\n        # Reading cells dataframe\n        cells_df = pd.read_parquet(pfm.cells_df)\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        # Grouping cells by region name\n        cells_agg_df = cells_df.groupby(AnnotColumns.ID.value).agg(CELL_AGG_MAPPINGS)\n        cells_agg_df.columns = list(CELL_AGG_MAPPINGS.keys())\n        # Reading annotation mappings dataframe\n        # Making df of region names and their parent region names\n        annot_df = annot_dict2df(read_json(pfm.map))\n        # Combining (summing) the cells_groagg values for parent regions using the annot_df\n        cells_agg_df = combine_nested_regions(cells_agg_df, annot_df)\n        # Calculating integrated average intensity (sum_intensity / volume)\n        cells_agg_df[CellColumns.IOV.value] = (\n            cells_agg_df[CellColumns.SUM_INTENSITY.value]\n            / cells_agg_df[CellColumns.VOLUME.value]\n        )\n        # Selecting and ordering relevant columns\n        cells_agg_df = cells_agg_df[[*ANNOT_COLUMNS_FINAL, *enum2list(CellColumns)]]\n        # Saving to disk\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_agg = dd.from_pandas(cells_agg)\n        cells_agg_df.to_parquet(pfm.cells_agg_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.make_mask_pipeline","title":"<code>make_mask_pipeline(pfm, overwrite=False)</code>","text":"<p>Makes mask of actual image in reference space. Also stores # and proportion of existent voxels for each region.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef make_mask_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Makes mask of actual image in reference space.\n    Also stores # and proportion of existent voxels\n    for each region.\n    \"\"\"\n    # Getting configs\n    configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n    # Reading ref and trimmed imgs\n    ref_arr = tifffile.imread(pfm.ref)\n    trimmed_arr = tifffile.imread(pfm.trimmed)\n    # Making mask\n    blur_arr = Gf.gauss_blur_filt(trimmed_arr, configs.mask_gaus_blur)\n    tifffile.imwrite(pfm.premask_blur, blur_arr)\n    mask_arr = Gf.manual_thresh(blur_arr, configs.mask_thresh)\n    tifffile.imwrite(pfm.mask, mask_arr)\n\n    # Make outline\n    outline_df = make_outline(mask_arr)\n    # Transformix on coords\n    outline_df[[Coords.Z.value, Coords.Y.value, Coords.X.value]] = (\n        transformation_coords(\n            outline_df,\n            pfm.ref,\n            pfm.regresult,\n        )[[Coords.Z.value, Coords.Y.value, Coords.X.value]]\n        .round(0)\n        .astype(np.int32)\n    )\n    # Filtering out of bounds coords\n    s = ref_arr.shape\n    outline_df = outline_df.query(\n        f\"\"\"\n        ({Coords.Z.value} &gt;= 0) &amp; ({Coords.Z.value} &lt; {s[0]}) &amp;\n        ({Coords.Y.value} &gt;= 0) &amp; ({Coords.Y.value} &lt; {s[1]}) &amp;\n        ({Coords.X.value} &gt;= 0) &amp; ({Coords.X.value} &lt; {s[2]})\n        \"\"\"\n    )\n\n    # Make outline img (1 for in, 2 for out)\n    # TODO: convert to return np.array and save out-of-function\n    coords2points(\n        outline_df[outline_df.is_in == 1],\n        ref_arr.shape,\n        pfm.outline,\n    )\n    in_arr = tifffile.imread(pfm.outline)\n    coords2points(\n        outline_df[outline_df.is_in == 0],\n        ref_arr.shape,\n        pfm.outline,\n    )\n    out_arr = tifffile.imread(pfm.outline)\n    tifffile.imwrite(pfm.outline, in_arr + out_arr * 2)\n\n    # Fill in outline to recreate mask (not perfect)\n    mask_reg_arr = fill_outline(outline_df, ref_arr.shape)\n    # Opening (removes FP) and closing (fills FN)\n    mask_reg_arr = ndimage.binary_closing(mask_reg_arr, iterations=2).astype(np.uint8)\n    mask_reg_arr = ndimage.binary_opening(mask_reg_arr, iterations=2).astype(np.uint8)\n    # Saving\n    tifffile.imwrite(pfm.mask_reg, mask_reg_arr)\n\n    # Counting mask voxels in each region\n    annot_arr = tifffile.imread(pfm.annot)\n    annot_df = annot_dict2df(read_json(pfm.map))\n    # Getting the annotation name for every cell (zyx coord)\n    mask_df = pd.merge(\n        left=mask2region_counts(np.full(annot_arr.shape, 1), annot_arr),\n        right=mask2region_counts(mask_reg_arr, annot_arr),\n        how=\"left\",\n        left_index=True,\n        right_index=True,\n        suffixes=(\"_annot\", \"_mask\"),\n    ).fillna(0)\n    # Combining (summing) the cells_agg_df values for parent regions using the annot_df\n    mask_df = combine_nested_regions(mask_df, annot_df)\n    # Calculating proportion of mask volume in each region\n    mask_df[MaskColumns.VOLUME_PROP.value] = (\n        mask_df[MaskColumns.VOLUME_MASK.value] / mask_df[MaskColumns.VOLUME_ANNOT.value]\n    )\n    # Selecting and ordering relevant columns\n    mask_df = mask_df[[*ANNOT_COLUMNS_FINAL, *enum2list(MaskColumns)]]\n    # Saving\n    mask_df.to_parquet(pfm.mask_df)\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.overwrite_check_decorator","title":"<code>overwrite_check_decorator(func)</code>","text":"<p>Decorator to check overwrite and will not run the function if the output file (as specified in <code>overwrite_fp_map</code>) already exists.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>def overwrite_check_decorator(func: Callable):\n    \"\"\"\n    Decorator to check overwrite and will\n    not run the function if the output file\n    (as specified in `overwrite_fp_map`) already exists.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Getting overwrite arg\n        overwrite = kwargs.get(\"overwrite\", False)\n        # If overwrite is False, check if output file exists\n        if not overwrite:\n            # Getting pfm arg\n            pfm = kwargs.get(\"pfm\", args[0])\n            # Iterating through filepaths that will be overwritten\n            # No checks if func name not in overwrite_fp_map\n            for fp in overwrite_fp_map.get(func.__name__, []):\n                if os.path.exists(getattr(pfm, fp)):\n                    logging.info(f\"Skipping {func.__name__} as {fp} already exists.\")\n                    return\n        # Running func\n        return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.tiff2zarr_pipeline","title":"<code>tiff2zarr_pipeline(pfm, in_fp, overwrite=False)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>pfm</code> <code>ProjFpModel</code> <p>description</p> required <code>in_fp</code> <code>str</code> <p>description</p> required <code>overwrite</code> <code>bool</code> <p>description, by default False</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef tiff2zarr_pipeline(\n    pfm: ProjFpModel,\n    in_fp: str,\n    overwrite: bool = False,\n):\n    \"\"\"\n    _summary_\n\n    Parameters\n    ----------\n    pfm : ProjFpModel\n        _description_\n    in_fp : str\n        _description_\n    overwrite : bool, optional\n        _description_, by default False\n\n    Raises\n    ------\n    ValueError\n        _description_\n    \"\"\"\n    # Getting configs\n    configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n    # Making zarr from tiff file(s)\n    with cluster_proc_contxt(LocalCluster(n_workers=1, threads_per_worker=6)):\n        if os.path.isdir(in_fp):\n            # If in_fp is a directory, make zarr from the tiff file stack in directory\n            tiffs2zarr(\n                in_fp_ls=natsorted(\n                    [\n                        os.path.join(in_fp, i)\n                        for i in os.listdir(in_fp)\n                        if re.search(r\".tif$\", i)\n                    ]\n                ),\n                out_fp=pfm.raw,\n                chunks=configs.chunksize,\n            )\n        elif os.path.isfile(in_fp):\n            # If in_fp is a file, make zarr from the btiff file\n            btiff2zarr(\n                in_fp=in_fp,\n                out_fp=pfm.raw,\n                chunks=configs.chunksize,\n            )\n        else:\n            raise ValueError(f'Input file path, \"{in_fp}\" does not exist.')\n</code></pre>"},{"location":"reference/pipeline_funcs.html#microscopy_proc.pipelines.pipeline_funcs.transform_coords_pipeline","title":"<code>transform_coords_pipeline(pfm, overwrite=False)</code>","text":"<p><code>in_id</code> and <code>out_id</code> are either maxima or region</p> <p>NOTE: saves the cells_trfm dataframe as pandas parquet.</p> Source code in <code>microscopy_proc/pipelines/pipeline_funcs.py</code> <pre><code>@overwrite_check_decorator\ndef transform_coords_pipeline(\n    pfm: ProjFpModel,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    `in_id` and `out_id` are either maxima or region\n\n    NOTE: saves the cells_trfm dataframe as pandas parquet.\n    \"\"\"\n    # Getting configs\n    configs = ConfigParamsModel.model_validate(read_json(pfm.config_params))\n    with cluster_proc_contxt(LocalCluster(n_workers=4, threads_per_worker=1)):\n        # Setting output key (in the form \"&lt;maxima/region&gt;_trfm_df\")\n        # Getting cell coords\n        cells_df = dd.read_parquet(pfm.cells_raw_df).compute()\n        # Sanitising (removing smb columns)\n        cells_df = sanitise_smb_df(cells_df)\n        # Taking only Coords.Z.value, Coords.Y.value, Coords.X.value coord columns\n        cells_df = cells_df[[Coords.Z.value, Coords.Y.value, Coords.X.value]]\n        # Scaling to resampled rough space\n        # NOTE: this downsampling uses slicing so must be computed differently\n        cells_df = cells_df / np.array(\n            (configs.z_rough, configs.y_rough, configs.x_rough)\n        )\n        # Scaling to resampled space\n        cells_df = cells_df * np.array((configs.z_fine, configs.y_fine, configs.x_fine))\n        # Trimming/offsetting to sliced space\n        cells_df = cells_df - np.array(\n            [\n                s[0] if s[0] else 0\n                for s in (configs.z_trim, configs.y_trim, configs.x_trim)\n            ]\n        )\n\n        cells_trfm_df = transformation_coords(cells_df, pfm.ref, pfm.regresult)\n        # NOTE: Using pandas parquet. does not work with dask yet\n        # cells_df = dd.from_pandas(cells_df, npartitions=1)\n        # Fitting resampled space to atlas image with Transformix (from Elastix registration step)\n        # cells_df = cells_df.repartition(\n        #     npartitions=int(np.ceil(cells_df.shape[0].compute() / ROWSPPART))\n        # )\n        # cells_df = cells_df.map_partitions(\n        #     transformation_coords, pfm.ref, pfm.regresult\n        # )\n        cells_trfm_df.to_parquet(pfm.cells_trfm_df)\n</code></pre>"},{"location":"tutorials/configs.html","title":"Configs JSON File","text":"<p>A configs JSON file is attached to each experiment. This file defines a) how the experiment should be processed (e.g. hyperparameters like the <code>dlc_config_fp</code> to use), and b) the inherent parameters of the experiment (e.g. like the <code>px/mm</code> and <code>start_frame</code> calculations).</p> <p>An example configs file is shown below:</p> <pre><code>{\n  \"user\": {\n    \"format_vid\": {\n      \"height_px\": 540,\n      \"width_px\": 960,\n      \"fps\": 15,\n      \"start_sec\": null,\n      \"stop_sec\": null\n    },\n    \"run_dlc\": {\n      \"model_fp\": \"/path/to/dlc_config.yaml\"\n    },\n    \"calculate_params\": {\n      \"start_frame\": {\n        \"window_sec\": 1,\n        \"pcutoff\": 0.9,\n        \"bodyparts\": \"--bodyparts-simba\"\n      },\n      \"exp_dur\": {\n        \"window_sec\": 1,\n        \"pcutoff\": 0.9,\n        \"bodyparts\": \"--bodyparts-simba\"\n      },\n      \"stop_frame\": {\n        \"dur_sec\": 6000\n      },\n      \"px_per_mm\": {\n        \"pt_a\": \"--tl\",\n        \"pt_b\": \"--tr\",\n        \"dist_mm\": 400\n      }\n    },\n    \"preprocess\": {\n      \"interpolate\": {\n        \"pcutoff\": 0.5\n      },\n      \"bodycentre\": {\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"refine_ids\": {\n        \"marked\": \"mouse1marked\",\n        \"unmarked\": \"mouse2unmarked\",\n        \"marking\": \"AnimalColourMark\",\n        \"window_sec\": 0.5,\n        \"metric\": \"rolling\",\n        \"bodyparts\": \"--bodyparts-centre\"\n      }\n    },\n    \"evaluate\": {\n      \"keypoints_plot\": {\n        \"bodyparts\": [\"Nose\", \"BodyCentre\", \"TailBase1\"]\n      },\n      \"eval_vid\": {\n        \"funcs\": [\"keypoints\", \"behavs\"],\n        \"pcutoff\": 0.5,\n        \"colour_level\": \"individuals\",\n        \"radius\": 4,\n        \"cmap\": \"rainbow\"\n      }\n    },\n    \"extract_features\": {\n      \"individuals\": [\"mouse1marked\", \"mouse2unmarked\"],\n      \"bodyparts\": \"--bodyparts-simba\"\n    },\n    \"classify_behaviours\": [\n      {\n        \"model_fp\": \"/path/to/behav_model_1.json\",\n        \"pcutoff\": null,\n        \"min_window_frames\": \"--min_window_frames\",\n        \"user_behavs\": \"--user_behavs\"\n      },\n      {\n        \"model_fp\": \"/path/to/behav_model_2.json\",\n        \"pcutoff\": null,\n        \"min_window_frames\": \"--min_window_frames\",\n        \"user_behavs\": \"--user_behavs\"\n      }\n    ],\n    \"analyse\": {\n      \"thigmotaxis\": {\n        \"thresh_mm\": 50,\n        \"roi_top_left\": \"--tl\",\n        \"roi_top_right\": \"--tr\",\n        \"roi_bottom_left\": \"--bl\",\n        \"roi_bottom_right\": \"--br\",\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"center_crossing\": {\n        \"thresh_mm\": 125,\n        \"roi_top_left\": \"--tl\",\n        \"roi_top_right\": \"--tr\",\n        \"roi_bottom_left\": \"--bl\",\n        \"roi_bottom_right\": \"--br\",\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"in_roi\": {\n        \"thresh_mm\": 5,\n        \"roi_top_left\": \"--tl\",\n        \"roi_top_right\": \"--tr\",\n        \"roi_bottom_left\": \"--bl\",\n        \"roi_bottom_right\": \"--br\",\n        \"bodyparts\": [\"Nose\"]\n      },\n      \"speed\": {\n        \"smoothing_sec\": 1,\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"social_distance\": {\n        \"smoothing_sec\": 1,\n        \"bodyparts\": \"--bodyparts-centre\"\n      },\n      \"freezing\": {\n        \"window_sec\": 2,\n        \"thresh_mm\": 5,\n        \"smoothing_sec\": 0.2,\n        \"bodyparts\": \"--bodyparts-simba\"\n      },\n      \"bins_sec\": [30, 60, 120, 300],\n      \"custom_bins_sec\": [60, 120, 300, 600]\n    }\n  },\n  \"ref\": {\n    \"bodyparts-centre\": [\n      \"LeftFlankMid\",\n      \"BodyCentre\",\n      \"RightFlankMid\",\n      \"LeftFlankRear\",\n      \"RightFlankRear\",\n      \"TailBase1\"\n    ],\n    \"bodyparts-simba\": [\n      \"LeftEar\",\n      \"RightEar\",\n      \"Nose\",\n      \"BodyCentre\",\n      \"LeftFlankMid\",\n      \"RightFlankMid\",\n      \"TailBase1\",\n      \"TailTip4\"\n    ],\n    \"tl\": \"TopLeft\",\n    \"tr\": \"TopRight\",\n    \"bl\": \"BottomLeft\",\n    \"br\": \"BottomRight\",\n    \"min_window_frames\": 2,\n    \"user_behavs\": [\"fight\", \"aggression\"]\n  }\n}\n</code></pre>"},{"location":"tutorials/configs.html#the-structure","title":"The Structure","text":"<p>The configs file has three main sections - <code>user</code>: User defined parameters to process the experiment. - <code>auto</code>: Automatically calculated parameters which are used     in later processes for the experiment.     Also gives useful insights into how the experiment \"went\" (e.g. over/under time, arena is smaller than other videos). - <code>ref</code>: User defined parameters can be referenced from keys defined here.     Useful when the same parameter values are used for many processes     (e.g. bodyparts).</p>"},{"location":"tutorials/configs.html#understanding-specific-parameters","title":"Understanding Specific Parameters","text":"<p>Notes</p> <p>To understand specific parameters in the <code>configs.yaml</code>, see each processing function's API documentation.</p> <p>For example, <code>user.calculate_params.px_per_mm</code> requires <code>pt_a</code>, <code>pt_b</code>, and <code>dist_mm</code>, which are described in the API docs.</p>"},{"location":"tutorials/configs.html#the-ref-section","title":"The Ref section","text":"<p>The <code>ref</code> section defines values that can be referenced in the <code>user</code> section.</p> <p>To reference a value from the ref section, first define it:</p> <pre><code>{\n    ...\n    \"ref\": {\n        \"example\": [\"values\", \"of\", \"any\", \"type\"]\n    }\n}\n</code></pre> <p>You can now reference <code>example</code> by prepending a double hyphen (<code>--</code>) when referencing it:</p> <pre><code>{\n    \"user\": {\n        ...\n        \"parameter\": \"--example\",\n        ...\n    },\n    ...\n}\n</code></pre>"},{"location":"tutorials/configs.html#setting-the-configs-for-an-experiment-or-all-experiments-in-a-project","title":"Setting the Configs for an Experiment or all Experiments in a Project","text":"<p>Each experiment requires a corresponding configs file.</p> <p>To generate or modify an experiment's configs file, first make a <code>default.json</code> file with the configs configured as you'd like.</p> <p>Tip</p> <p>You can copy the example configs file from here.</p> <p>Just make sure to change the multiple <code>model_fp</code> filepaths.</p> <pre><code>from behavysis_pipeline import Experiment\n\n# Getting the experiment\nexperiment = Experiment(\"exp_name\", \"root_dir\")\n# Making/overwriting the configs file\nexperiment.update_configs(\"/path/to/default.json\", overwrite=\"all\")\n</code></pre> <p>Note</p> <p>The <code>overwrite</code> keyword can be <code>\"all\"</code>, or <code>\"user\"</code>.</p>"},{"location":"tutorials/tutorial.html","title":"Setup","text":"<p>Before running the behavysis_pipeline analysises, the files that we want to analyse must be set up a certain way for the behavysis_pipeline program to recognise them.</p> <p>There are three important guidelines to set up the project:</p> <ul> <li>Structure of files in folders .</li> <li>Experiment files.</li> <li>Config files for each experiment.</li> </ul>"},{"location":"tutorials/tutorial.html#folder-structure","title":"Folder Structure","text":"<p>They need to be set up inside specially named folders, as shown below.</p> <p>An example of how this would look on a computer (in this case, a Mac) is shown below.</p>"},{"location":"tutorials/tutorial.html#experiment-files","title":"Experiment Files","text":"<p>Each experiment must have files that have same name (not including the suffix like <code>.csv</code> or <code>.mp4</code>). An example is \"day1_experiment1\" must have all files named \"day1_experiment1.mp4\", \"day1_experiment1.csv\", \"day1_experiment1.json\" etc. stored in the corresponding folder.</p>"},{"location":"tutorials/tutorial.html#config-files","title":"Config Files","text":"<p>The config file for an experiment stores all the parameters for how the experiment was recorded (e.g., the frames per second of the raw video, the experiment duration, etc.), and the parameters for how we want to process the data (e.g., the intended frames per second to format the video to, the DLC model to use to analyse, the likeliness pcutoff to interpolate points, etc.)</p> <p>An example of a config file is shown here.</p>"},{"location":"tutorials/tutorial.html#running-behavysis_pipeline","title":"Running behavysis_pipeline","text":"<p>To install <code>behavysis_pipeline</code>, follow these instructions.</p> <p>To run <code>behavysis_pipeline</code>, follow these these instructions.</p>"}]}